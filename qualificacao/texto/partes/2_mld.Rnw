\chapter{Modelo Linear Dinâmico} \label{mld}
%
<<mld-setup, echo = FALSE, include = FALSE>>=
rnd_mld <- function(n, F, G, V, W, m0, C0) {
    theta_t <- matrix(MASS::mvrnorm(1, m0, C0), length(m0), 1)
    theta_t <- matrix(MASS::mvrnorm(1, G %*% theta_t, W), length(theta_t), 1)
    Y_t <- rnorm(n, t(F) %*% theta_t, sqrt(V))

    for (i in 2:n) {
        theta_t <- matrix(MASS::mvrnorm(1, G %*% theta_t, W),
                          length(theta_t), 1)
        Y_t[i] <- rnorm(1, t(F) %*% theta_t, sqrt(V))
    }
    return(Y_t)
}

n <- 100
F <- matrix(c(1, pi), 2, 1)
G <- matrix(c(0.15, -0.08, -0.08, 0.15), 2, 2)
V <- matrix(c(1), 1, 1)
W <- matrix(c(1, 0.3, 0.3, 1), 2, 2)
m0 <- matrix(c(0, 0), 2, 1)
C0 <- matrix(c(10, 0, 0, 10), 2, 2)
@
%
O uso de modelos lineares dinâmicos (MLD) cresceu bastante nas últimas décadas. Sua aplicabilidade tem se estendido nos campos da biologia, genética, geofísica, economia entre outros, \citep{petrone}. O desenvolvimento computacional das técnicas Bayesianas de estimação dos parâmetros é um dos fatores que impulsionou esse crescimento recente.

Eles constituem uma família muito importante, pois, além de trazerem os modelos tradicionais de séries de tempo para a abordagem Bayesiana, expandem a gama de possibilidades de aplicações através da sua estrutura flexível e do próprio paradigma Bayesiano em si. São, ainda, um caso particular linear e gaussiano dos modelos mais gerais de espaço-estado.

\section{Definição do Modelo}

O modelo linear dinâmico mais geral é definido em relação a um vetor de observações $\vetor{Y}_t$, porém, para efeitos de simplificação, será considerado o caso univariado $Y_t$ como em \cite{west}. Portanto, sejam $Y_t$ uma variável observada de interesse, $\vetor{\theta}_t$ um vetor latente de variáveis que representam o sistema gerador de $Y_t$ e $D_t$ toda informação disponível a respeito do sistema. Desse modo, para todo tempo $t$, o modelo linear dinâmico é:
%
\begin{align}
	&\text{\textbf{Equação das Observações:}} & Y_t &= \vetor{F}'_t\vetor{\theta}_t + \nu_t, & \nu_t &\sim \dnormal{0}{V_t}{}, \label{mld:eq_obs}\\
	&\text{\textbf{Equação do Sistema:}} & \vetor{\theta}_t &= \vetor{G}_t\vetor{\theta}_{t - 1} + \vetor{\omega}_t, & \vetor{\omega}_t &\sim \dnormal{\vetor{0}}{\vetor{W}_t}{}. \label{mld:eq_sis}
\end{align}

A quádrupla $\{\vetor{F}_t, \vetor{G}_t, V_t, \vetor{W}_t\}$ é composta pelas matrizes que caracterizam o modelo. $\vetor{F}_t$ é a matriz de \textit{design}, ou o vetor de regressão, no caso univariado, $\vetor{G}_t$ é a matriz de evolução do sistema, $V_t$ é a variância observacional e $\vetor{W}_t$ é a matriz de variância da evolução do sistema. Se esses elementos forem invariantes no tempo, o modelo $\{\vetor{F}, \vetor{G}, V, \vetor{W}\}$ é chamado de constante, e engloba essencialmente todos os modelos lineares tradicionais de séries temporais.

Os termos $\nu_t$ e $\vetor{\omega}_t$ são os erros associados às equações \eqref{mld:eq_obs} e \eqref{mld:eq_sis}. Por definição, eles são independentes, no tempo e entre si, e apresentam distribuição normal com média zero. Modelos ainda mais gerais podem ser definidos com $\nu_t$ e $\vetor{\omega}_t$ autocorrelacionados e correlacionados entre si, \cite{west}, entretanto, destaca que tais modelos podem sempre ser reescritos em termos mais simples satisfazendo as condições de independência. Eles são denominados erro observacional e erro do sistema, respectivamente.

A informação inicial sobre $\vetor{\theta}_t$ é representada por:
%
\begin{equation}
	\text{\textbf{Informação Inicial:}} \qquad \pxt{\vetor{\theta}_0 | D_0} \sim \dnormal{\vetor{m}_0}{\vetor{C}_0}{}, \label{mld:eq_priori}\\
\end{equation}
%
onde $\vetor{m}_0$ e $\vetor{C}_0$ são o vetor de médias e a matriz de variâncias e covariâncias da distribuição proposta, respectivamente.

A Figura \ref{mld:fig_ex} mostra um exemplo de \Sexpr{n} observações simuladas a partir de um modelo linear dinâmico constante definido pela quádrupla:
%
\begin{equation}
  \{\vetor{F}, \vetor{G}, V, \vetor{W}\} = \left\{\begin{bmatrix}1 \\ \pi\end{bmatrix} , \begin{bmatrix}0,15 & -0,08 \\ -0,08 & 0,15 \end{bmatrix}, 1, \begin{bmatrix}1 & 0,3 \\ 0,3 & 1\end{bmatrix}\right\}. \label{mld:matrizes_exemplo}
\end{equation}
%
A série partiu da distribuição inicial:
%
\begin{equation}
	\pxt{\vetor{\theta}_0 | D_0} \sim \dnormalxt{\begin{bmatrix}0 \\ 0\end{bmatrix}}{\begin{bmatrix}10 & 0 \\ 0 & 10\end{bmatrix}}{}. \label{mld:priori_exemplo}\\
\end{equation}
%
\begin{figure}[ht]
\centering
\begin{minipage}{0.8\linewidth}
<<ex-mld, eval = TRUE, echo = FALSE, opts.label = "regular_fig">>=
data.frame(t = 1:n, y_t = rnd_mld(n, F, G, V, W, m0, C0)) %>%
    ggplot(aes(x = t, y = y_t)) +
    geom_line(size = 0.5) +
    geom_point(size = 1.5) +
    labs(x = "$t$", y = "$y_t$")
@
\caption{Exemplo simulado de \Sexpr{n} observações geradas a partir de um modelo polinomial de primeira ordem.}
\label{mld:fig_ex}
\end{minipage}
\end{figure}
%
Os valores dos parâmetros nesse exemplo foram selecionados arbitrariamente, apenas tomando-se o cuidado para que não fossem muito grandes de modo a gerar observações numa escala elevada.

Dentre as finalidades de um modelo de dados temporais está a previsão de observações futuras. Sob esse aspecto, o objetivo a ser alcançado agora é definir as distribuições de probabilidade de $\pxt{Y_t | D_{t - 1}}$ e $\pxt{\vetor{\theta}_t | D_t}$. A primeira é a distribuição da previsão de $Y$ no tempo $t$ dado todo o conhecimento anterior a esse tempo. A outra é a distribuição do vetor $\vetor{\theta}$ no tempo $t$ dado todo conhecimento disponível até então. Antes da definição dessas distribuições, deve ser apresentado um breve resumo por trás da ideia do teorema de Bayes que é fundamental no desenvolvimento dos próximos passos.

\section{Breve Resumo do Teorema de Bayes}

Sejam dois eventos distintos $A$ e $B$, a probabilidade de ocorrência conjunta deles é dada pela regra do produto:
%
\begin{equation}
  P(A,B) = P(A|B)P(B). \label{mld:regra_produto}
\end{equation}
%
Naturalmente, a ordem dos eventos pode ser mudada, isto é, $P(B,A) = P(B|A)P(A)$. A partir dessas duas equações surge a relação:
%
\begin{equation}
  P(A|B) = \frac{P(B|A) P(A)}{P(B)}. \label{mld:teo_bayes}
\end{equation}
%
O resultado em \eqref{mld:teo_bayes} tem sua origem nas ideias do Reverendo Thomas Bayes no século XVIII, porém Pierre Simon de Laplace é o autor dessa equação conhecida como teorema de Bayes, \cite{sivia}. No contexto dos modelos lineares dinâmicos,
%
\begin{equation}
f(\vetor{\theta}_t | y_t, D_{t-1}) = \frac{g(y_t | \vetor{\theta}_t, D_{t-1}) \pi(\vetor{\theta}_t | D_{t-1})}{h(y_t|D_{t-1})}.\label{mld:eq_bayes}
\end{equation}

O processo latente definido por $\vetor{\theta}_t$ é estimado através da realização $y_t$ de $Y$ e do conjunto de informações relevantes, $D_{t -1}$. Porém, a incerteza (ou certeza) inicial existente sobre $\vetor{\theta}_t$ deve ser expressa através de uma distribuição de probabilidade adequada \citep{jaynes}, no caso, $\pi(\vetor{\theta}_t|D_{t-1})$. Ela é chamada de distribuição \textit{a priori} de $\pxt{\vetor{\theta}_t|D_{t-1}}$.

Conforme surjam novas realizações dos dados, o conhecimento sobre o processo latente deve ser atualizado. Isso é feito pela função $g(y_t | \vetor{\theta}_t, D_{t-1})$, denominada de verossimilhança.

A função $h(y_t | D_{t-1})$ é chamada de distribuição preditiva \textit{a priori} de $y_t$. Ela é muito útil quando é desejável fazer inferências a respeito de uma observação ainda desconhecida, \citep{gelman}. No caso, quando for o interesse estimar a, até então desconhecida, observação $y_t$ de $Y$.

Por fim, $f(\vetor{\theta}_t | y_t, D_{t-1})$ é a distribuição resultante da interação das três anteriores. Ela permite fazer inferências sobre o processo latente e recebe o nome de distribuição \textit{a posteriori} de $\pxt{\vetor{\theta}_t | D_t}$, uma vez que o conhecimento de $y_t$ e $D_{t-1}$ dá origem a $D_t$.

Recursivamente, a distribuição \textit{a posteriori} de $\pxt{\vetor{\theta}_t | D_t}$ pode se tornar a distribuição \textit{a priori} de $\pxt{\vetor{\theta}_{t + 1} | D_t}$. Assim o teorema de Bayes flui naturalmente no contexto dos modelos lineares dinâmicos. Como pode ser visto através das equações de atualização.

\section{Equações de Atualização}

Uma das motivações ao se propor um modelo de dados temporais é fazer predições futuras. O modelo linear dinâmico oferece um conjunto de equações que são atualizadas ao longo do tempo, e que permitem estimar essas informações. O raciocínio a seguir segue o proposto em \cite{west}. Apenas para simplificar a álgebra futura, será considerado o modelo linear dinâmico constante.

Seja, para algum $\vetor{m}_t$ e $\vetor{C}_t$, \textit{a posteriori} de $\vetor{\theta}_t$:
%
	\begin{equation}
  	\pxt{\vetor{\theta}_t | D_t} \sim \dnormal{\vetor{m}_t}{\vetor{C}_t}{}. \label{mld:eq_posteriori_theta_t}
  \end{equation}
%
\noindent A equação \eqref{mld:eq_sis} permite, então, calcular a distribuição \textit{a priori} de $\vetor{\theta}_{t+1}$ dada a informação em $t$ que é:
%
\begin{align}
 \vetor{\theta}_{t + 1} &= \vetor{G}\vetor{\theta}_t + \vetor{\omega}_{t + 1}, \notag \\
	\pxt{\vetor{\theta}_{t + 1} | D_t} &\sim \vetor{G}\dnormal{\vetor{m}_t}{\vetor{C}_t}{\pxt{\vetor{\theta}_t | D_t}} + \dnormal{\vetor{0}}{\vetor{W}}{\vetor{\omega}_{t + 1}}, \notag \\
	\pxt{\vetor{\theta}_{t + 1} | D_t} &\sim \dnormal{\vetor{a}_{t + 1}}{\vetor{R}_{t+1}}{}, \label{mld:eq_priori_theta_t+1}
\end{align}
%
\noindent onde $\vetor{a}_{t+1} = \vetor{G} \vetor{m}_t$ e $\vetor{R}_{t + 1} = \vetor{G}\vetor{C}_t\vetor{G}' + \vetor{W}$.

Semelhantemente, a equação \eqref{mld:eq_obs} é utilizada para calcular a distribuição da previsão da observação de $Y_{t + 1}$ dada a informação em $t$, ou a primeira previsão:
%
\begin{align}
  Y_{t + 1} &= \vetor{F}'\vetor{\theta}_{t + 1} + \nu_{t + 1}, \notag \\
	\pxt{Y_{t + 1} | D_t}	&\sim \vetor{F}'\dnormal{\vetor{a}_{t + 1}}{\vetor{R}_{t + 1}}{\pxt{\vetor{\theta}_{t + 1} | D_t}} + \dnormal{0}{V}{\nu_{t + 1}}, \notag \\
	\pxt{Y_{t + 1} | D_t} &\sim \dnormal{f_{t + 1}}{Q_{t + 1}}{}, \label{mld:eq_forecast_1}
\end{align}
%
\noindent onde $f_{t + 1} = \vetor{F}'\vetor{a}_{t + 1}$ e $Q_{t + 1} = \vetor{F}'\vetor{R}_{t + 1}\vetor{F} + V$.

A distribuição \textit{a posteriori} de $\vetor{\theta}_{t + 1}$ dada a informação em $t + 1$, que será \textit{a priori} na próxima iteração, será:
%
\begin{equation}
  \pxt{\vetor{\theta}_{t + 1} | D_{t + 1}} \sim \dnormal{\vetor{m}_{t + 1}}{\vetor{C}_{t + 1}}{}, \label{mld:eq_posteriori_theta_t+1}
\end{equation}
%
onde, $\vetor{m}_{t + 1} = \vetor{a}_{t + 1} + \vetor{A}_{t + 1} e_{t + 1}$ e $\vetor{C}_{t + 1} = \vetor{R}_{t + 1} - \vetor{A}_{t + 1} Q_{t + 1} \vetor{A}'_{t + 1}$, com $\vetor{A}_{t + 1} = \vetor{R}_{t + 1}\vetor{F}Q^{-1}_{t + 1}$ e $e_{t + 1} = Y_{t + 1} - f_{t + 1}$.

\section{Previsões}

A previsão para o primeiro passo a frente é dada pela distribuição \eqref{mld:eq_forecast_1} descrita anteriormente. Para definir o k-ésimo valor predito, é necessário antes encontrar a distribuição de $\vetor{\theta}_{t + k}$. Isso é feito a partir da distribuição \textit{a priori} \eqref{mld:eq_priori_theta_t+1} aplicada na equação do sistema \eqref{mld:eq_sis} sucessivamente. Assim, para $D_t$ que é ainda a última informação disponível e para $k = 2$:
%
\begin{align}
  \vetor{\theta}_{t + 2} &= \vetor{G}\vetor{\theta}_{t + 1} + \vetor{\omega}_{t + 2}, \notag \\
  \pxt{\vetor{\theta}_{t + 2} | D_t}	&\sim \vetor{G}\dnormal{\vetor{a}_{t + 1}}{\vetor{R}_{t + 1}}{\pxt{\vetor{\theta}_{t + 1} | D_t}} + \dnormal{0}{\vetor{W}}{\vetor{\omega}_{t + 2}}, \notag \\
  \pxt{\vetor{\theta}_{t + 2} | D_t} &\sim \dnormal{\vetor{a}_{t + 2}}{\vetor{R}_{t + 2}}{}, \label{mld:eq_priori_theta+2}
\end{align}
%
\noindent onde $\vetor{a}_{t+2} = \vetor{G}\vetor{a}_{t + 1}$ e $\vetor{R}_{t + 2} = \vetor{G}\vetor{R}_{t + 1}\vetor{G}' + \vetor{W}$.

Para um valor de $k \geq 2$, \cite{pole} mostra que:
%
\begin{equation}
 \pxt{\vetor{\theta}_{t + k} | D_t} \sim \dnormal{\vetor{a}_{t + k}}{\vetor{R}_{t + k}}{}, \label{mld:eq_priori_theta+k}
\end{equation}
%
\noindent onde $\vetor{a}_{t+k} = \vetor{G}^{k - 1}\vetor{a}_{t + 1}$ e $\vetor{R}_{t + k} = \vetor{G}^{k - 1}\vetor{R}_{t + 1}\pxt{\vetor{G}^{k - 1}}' + \soma{j = 2}{k}\vetor{G}^{k - j}\vetor{W}\pxt{\vetor{G}^{k - j}}'$. E, com isso, a distribuição da k-ésima predição dada a informação $D_t$ será:
%
\begin{equation}
 \pxt{Y_{t + k} | D_t} \sim \dnormal{f_{t + k}}{Q_{t + k}}{}, \label{mld:eq_forecast_k}
\end{equation}
%
\noindent onde $f_{t+k} = \vetor{F}'\vetor{a}_{t + k}$ e $Q_{t + k} = \vetor{F}'\vetor{R}_{t + k}\vetor{F}' + V$.
