\documentclass{beamer}
\usepackage[brazilian]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath, amsfonts, amssymb, amsopn, amsthm}
\usepackage[round, authoryear]{natbib}
\usetheme{Singapore}

\usepackage{icomma}
\usepackage{bm}
\usepackage{ragged2e}

\title[Defesa de Mestrado]{Um Estudo sobre Modelos para Volatilidade Estocástica}
\subtitle{Dissertação de Mestrado}
\author[André Queiroz]{André Silva de Queiroz \\ {\footnotesize Orientadora: Prof.$^a$ Dr.$^a$ Cibele Queiroz da Silva}}
\institute{Departamento de Estatística \\ Universidade de Brasília}
\date{10 de dezembro de 2015}

%Logo da UnB
\pgfdeclareimage[height=0.5cm]{unb}{unb}
\logo{\pgfuseimage{unb}}

\setbeamercovered{transparent}

\newcommand{\pxt}[1]{\left(#1\right)}
\newcommand{\soma}[2]{\sum\limits_{#1}^{#2}}
\newcommand{\abs}[1]{\left|#1\right|}

\newcommand{\E}[1]{\text{E}(#1)}
\newcommand{\var}[1]{\text{Var}(#1)}
\newcommand{\cov}[1]{\text{Cov}(#1)}

\newcommand{\dnormal}[3]{\mathcal{N}_{#3}(#1, #2)}
\newcommand{\dnormalxt}[3]{\mathcal{N}_{#3}\pxt{#1, #2}}
\newcommand{\dbeta}[3]{\mathcal{B}_{#3}(#1, #2)}
\newcommand{\dbetaxt}[3]{\mathcal{B}_{#3}\pxt{#1, #2}}
\newcommand{\dgamma}[3]{\mathcal{G}_{#3}(#1, #2)}
\newcommand{\dgammaxt}[3]{\mathcal{G}_{#3}\pxt{#1, #2}}

\newcommand{\vetor}[1]{\bm{#1}}

<<setup, echo = FALSE, include = FALSE>>=
library(DBI)
library(dplyr)
library(ggplot2)
library(grid)
library(knitr)
library(nor1mix)
library(tikzDevice)

set.seed(2347)
options(OutDec = ",", tikzDefaultEngine = "xetex")

theme_set(theme_bw())
theme_update(panel.border = element_blank(), axis.line = element_line(),
             axis.title = element_text(size = 18),
             axis.title.y = element_text(vjust = 1.2, angle = 90),
             axis.text = element_text(size = 14),
             legend.text = element_text(size = 18),
             legend.title = element_text(size = 18),
             legend.key.size = unit(0.8, "cm"),
             legend.background = element_blank(),
             strip.text = element_text(size = 18),
             title = element_text(size = 16))

opts_template$set(regular_fig = list(dev = "tikz",
                                     R.options = list(OutDec = ","),
                                     fig.width = 9, fig.height = 4,
                                     fig.align = "center"),
                  triple_fig = list(dev = "tikz",
                                    R.options = list(OutDec = ","),
                                    fig.width = 8, fig.height = 3,
                                    fig.align = "center"),
                  single_fig = list(dev = "tikz",
                                    R.options = list(OutDec = ","),
                                    fig.width = 9, fig.height = 2.2,
                                    fig.align = "center"))

knit_hooks$set(document = function(x) {
  sub("\\usepackage[]{color}", "\\usepackage{xcolor}", x, fixed = TRUE)
})

link <- dbConnect(RSQLite::SQLite(), "../dados/svsim2.db")
link2 <- dbConnect(RSQLite::SQLite(), "../dados/svsim_resultados.db")

## gráficos
comp_h <- function(fase) {
    h_real <- dados_sim %>%
        filter(ind == "b") %>%
        select(t, values)

    paste("SELECT t, media
           FROM h
           WHERE fase =", fase) %>%
        dbGetQuery(link2, .) %>%
        inner_join(h_real, by = "t") %>%
        tidyr::gather(variable, value, -t) %>%
        ggplot(aes(x = t, y = value, col = variable)) +
        geom_line(size = 0.5) +
        labs(x = "$t$", y = "") +
        scale_colour_manual("", labels = c("$\\bar{\\hat{h}}_t$", "$h_t$"),
                            values = c("red", "black")) +
        theme(legend.position = c(0.9, 0.25)) -> g

    plot(g)
}

comp_jags <- function(vp, vs, vk) {

    tmp <- dbGetQuery(link2, "SELECT * FROM resultados") %>%
        filter(p == vp, s == vs, k == vk) %>%
        select(mu, phi, sigma2) %>%
        bind_rows(dbGetQuery(link, "SELECT * FROM jags") %>%
                  filter(p == vp, s == vs, k == vk) %>%
                  select(mu, phi, sigma2)) %>%
        mutate(origem = rep(c("ASIS", "JAGS"), each = 1000))

    ggplot(tmp, aes(x = mu, fill = origem)) +
        geom_density(alpha = 0.4) +
        labs(x = "$\\mu$", y = "") +
        geom_vline(xintercept = -5.4, colour = "red") +
        scale_fill_manual("", labels = c("ASIS", "JAGS"),
                          values = c("black", "blue")) +
        theme(legend.position = c(0.9, 0.75)) -> g
    plot(g)

    ggplot(tmp, aes(x = phi, fill = origem)) +
        geom_density(alpha = 0.4) +
        labs(x = "$\\phi$", y = "") +
        geom_vline(xintercept = vp, colour = "red") +
        scale_fill_manual("", labels = c("ASIS", "JAGS"),
                          values = c("black", "blue")) +
        theme(legend.position = c(0.9, 0.75)) -> g
    plot(g)

    ggplot(tmp, aes(x = sigma2, fill = origem)) +
        geom_density(alpha = 0.4) +
        labs(x = "$\\sigma_\\eta^2$", y = "") +
        geom_vline(xintercept = vs ** 2, colour = "red") +
        scale_fill_manual("", labels = c("ASIS", "JAGS"),
                          values = c("black", "blue")) +
        theme(legend.position = c(0.9, 0.75))
}
@

\begin{document}

%Capa
\frame{\titlepage}

\section*{Introdução}

\subsection*{Introdução}

\begin{frame}{Introdução}

  \begin{itemize}[<+->]
  
    \item A importância dos modelos de séries temporais.
    
    \item Dados com a variância variável:
    
      \begin{itemize}
      
        \item Tradicionalmente: ARCH e GARCH.
        
        \item Nova proposta: Modelos de Volatilidade Estocástica.
      
      \end{itemize}
      
     \item Inferência Bayesiana.
     
     \item Modelos Dinâmicos.
  
  \end{itemize}

\end{frame}

\section*{MLD}

\subsection*{MLD}

\begin{frame}{Modelo Linear Dinâmico (MLD)}

  \begin{itemize}[<+->]
  
    \item Família mais simples dentre os modelos dinâmicos.
    
    \item Segundo \cite{west} a definição do modelo linear dinâmico univariado é:
    
    	\begin{itemize}
    	
    	  \item \textbf{Equação das Observações:}
    	  
\begin{equation}
  Y_t = \vetor{F}'_t\vetor{\theta}_t + \nu_t, \qquad \nu_t \sim \dnormal{0}{V_t}{}, \label{mld:eq_obs}
\end{equation}

          \item \textbf{Equação do Sistema:}
    	  
\begin{equation}
  \vetor{\theta}_t = \vetor{G}_t\vetor{\theta}_{t - 1} + \vetor{\omega}_t, \qquad \vetor{\omega}_t \sim \dnormal{\vetor{0}}{\vetor{W}_t}{}. \label{mld:eq_sis}
\end{equation}

    	\end{itemize}
        
        \item O MLD é caracterizado pela quádrupla $\{\vetor{F}_t, \vetor{G}_t, V_t, \vetor{W}_t\}$.
  
  \end{itemize}

\end{frame}

\begin{frame}

  \begin{itemize}[<+->]

    \item Se $\{\vetor{F}, \vetor{G}, V, \vetor{W}\}$, então o modelo é chamado de constante.
    
    \item O modelo linear dinâmico constante engloba essencialmente todos os modelos lineares tradicionais de séries temporais.
    
    \item Modelos mais gerais podem ser propostos com $\nu_t$ e $\vetor{\omega}_t$ correlacionados. Porém esses novos modelos podem sempre ser reescritos satisfazendo as condições de independência, \cite{west}.
    
    \item A informação inicial sobre $\vetor{\theta}_t$ é representada por:
    
      \begin{itemize}
    	
    	\item \textbf{Informação Inicial:}
    	  
\begin{equation}
  \pxt{\vetor{\theta}_0 | D_0} \sim \dnormal{\vetor{m}_0}{\vetor{C}_0}{}. \label{mld:eq_priori}
\end{equation}

      \end{itemize}

  \end{itemize}

\end{frame}

\begin{frame}{O Teorema de Bayes}
  
  \begin{itemize}[<+->]
    
    \item A probabilidade de ocorrência conjunta de dois eventos distintos é dada pela regra do produto:
      %
      \begin{equation}
        P(A,B) = P(A|B)P(B). \label{mld:regra_produto}
      \end{equation}
      
   \item Teorema de Bayes:
     %
     \begin{equation}
       P(A|B) = \frac{P(B|A) P(A)}{P(B)}. \label{mld:teo_bayes}
     \end{equation}
     
   \item No contexto dos modelos lineares dinâmicos,
     %
     \begin{equation}
       f(\vetor{\theta}_t | D_t) = f(\vetor{\theta}_t | y_t, D_{t-1}) = \frac{g(y_t | \vetor{\theta}_t, D_{t-1}) \pi(\vetor{\theta}_t | D_{t-1})}{h(y_t|D_{t-1})}.\label{mld:eq_bayes}
     \end{equation}

  \end{itemize}
  
\end{frame}

\begin{frame}{Equações de Atualização}

  \begin{itemize}[<+->]
  
    \item Seja, para algum $\vetor{m}_t$ e $\vetor{C}_t$, \textit{a posteriori} de $\vetor{\theta}_t$:
    
	\begin{equation}
  	\pxt{\vetor{\theta}_t | D_t} \sim \dnormal{\vetor{m}_t}{\vetor{C}_t}{}. \label{mld:eq_posteriori_theta_t}
  \end{equation}

   \item As equações \eqref{mld:eq_sis} e \eqref{mld:eq_posteriori_theta_t} permitem calcular a distribuição \textit{a priori} $(\vetor{\theta}_{t+1} | D_t)$:
%
\begin{align}
 \vetor{\theta}_{t + 1} &= \vetor{G}\vetor{\theta}_t + \vetor{\omega}_{t + 1}, \notag \\
	\pxt{\vetor{\theta}_{t + 1} | D_t} &\sim \vetor{G}\dnormal{\vetor{m}_t}{\vetor{C}_t}{\pxt{\vetor{\theta}_t | D_t}} + \dnormal{\vetor{0}}{\vetor{W}}{\vetor{\omega}_{t + 1}}, \notag \\
	\pxt{\vetor{\theta}_{t + 1} | D_t} &\sim \dnormal{\vetor{a}_{t + 1}}{\vetor{R}_{t+1}}{}, \label{mld:eq_priori_theta_t+1}
\end{align}

onde $\vetor{a}_{t+1} = \vetor{G} \vetor{m}_t$ e $\vetor{R}_{t + 1} = \vetor{G}\vetor{C}_t\vetor{G}' + \vetor{W}$.
  
  \end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}[<+->]

\item A equação \eqref{mld:eq_obs} é utilizada para calcular a distribuição da previsão da observação $(Y_{t + 1} | D_t)$:
%
\begin{align}
  Y_{t + 1} &= \vetor{F}'\vetor{\theta}_{t + 1} + \nu_{t + 1}, \notag \\
	\pxt{Y_{t + 1} | D_t}	&\sim \vetor{F}'\dnormal{\vetor{a}_{t + 1}}{\vetor{R}_{t + 1}}{\pxt{\vetor{\theta}_{t + 1} | D_t}} + \dnormal{0}{V}{\nu_{t + 1}}, \notag \\
	\pxt{Y_{t + 1} | D_t} &\sim \dnormal{f_{t + 1}}{Q_{t + 1}}{}, \label{mld:eq_forecast_1}
\end{align}

onde $f_{t + 1} = \vetor{F}'\vetor{a}_{t + 1}$ e $Q_{t + 1} = \vetor{F}'\vetor{R}_{t + 1}\vetor{F} + V$.

\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}[<+->]

\item A distribuição \textit{a posteriori} $(\vetor{\theta}_{t + 1} | D_{t + 1})$ é obtida via o Teorema de Bayes:
%
\begin{equation}
  f(\vetor{\theta}_{t + 1} | D_{t + 1}) \propto g(Y_{t+1}|\vetor{\theta}_{t+1}, D_t) \pi(\vetor{\theta}_{t+1} | D_t) \label{mld:bayesdenovo}
\end{equation}

\item Desse modo:
  %
\begin{equation}
  \pxt{\vetor{\theta}_{t + 1} | D_{t + 1}} \sim \dnormal{\vetor{m}_{t + 1}}{\vetor{C}_{t + 1}}{}, \label{mld:eq_posteriori_theta_t+1}
\end{equation}


onde, $\vetor{m}_{t + 1} = \vetor{a}_{t + 1} + \vetor{A}_{t + 1} e_{t + 1}$ e $\vetor{C}_{t + 1} = \vetor{R}_{t + 1} - \vetor{A}_{t + 1} Q_{t + 1} \vetor{A}'_{t + 1}$, com $\vetor{A}_{t + 1} = \vetor{R}_{t + 1}\vetor{F}Q^{-1}_{t + 1}$ e $e_{t + 1} = Y_{t + 1} - f_{t + 1}$, como mostra \cite{west}.

\end{itemize}

\end{frame}

\section*{MVE}

\subsection*{MVE1}

\begin{frame}{Modelo de Volatilidade Estocástica (MVE)}

\begin{itemize}[<+->]

\item Modelagem inicialmente proposta por \cite{taylor}.

\item \cite{kim} define o modelo canônico como:

\item

\begin{equation}
\text{\textbf{Modelo Canônico}}:
\begin{cases}
  Y_t &= e^{\frac{h_t}{2}} \delta_t, \\
  h_t &= \mu + \phi(h_{t - 1} - \mu) + \eta_t, \\
  h_0 &\sim \dnormalxt{\mu}{\frac{\sigma_\eta^2}{1 - \phi^2}}{}.
\end{cases} \label{mvs:mvs}
\end{equation}

\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}[<+->]

\item Os termos $\delta_t$ e $\eta_t$ são ruídos gaussianos, independentes no tempo e entre si, com distribuições:
%
\begin{equation}
  \delta_t \sim \dnormal{0}{1}{} \quad\text{e}\quad \eta_t \sim \dnormalxt{0}{\sigma_\eta^2}{}. \label{mvs:eq_erros}
\end{equation}

\item Observe que $(Y_t | h_t) \sim \dnormalxt{0}{e^{h_t}}{}$.

\item O conjunto de parâmetros do modelo a serem estimados é definido pelo vetor:
%
\begin{equation}
  \vetor{\psi} = (\mu, \phi, \sigma_\eta^2). \label{mvs:psi}
\end{equation}

\end{itemize}

\end{frame}

\begin{frame}{Um Modelo de Espaço-Estado}

\begin{itemize}[<+->]

\item Os MVE podem ser reescritos na notação de espaço-estado apresentada por \cite{west} estendendo-se a proposta de \cite{zivot}.

\item Pela definição em \eqref{mvs:mvs}:

\begin{align}
  Y_t &= e^{\frac{h_t}{2}} \delta_t, \notag \\
  \ln Y_t^2 &= h_t + \ln\delta_t^2, \notag \\
  \ln Y_t^2 &= h_t + \E{\ln \delta_t^2} + \varepsilon_t. \label{mvs:eq_obs_reescrita_ingenua}
\end{align}

Assim $\varepsilon_t \sim (0, \var{\ln\delta_t^2})$ é um ruído em torno de zero.

\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}[<+->]

\item \cite{koopman} afirmam que isso não impede a utilização das técnicas associadas aos modelos gaussianos.

\item \cite{omori} sugere uma mistura de $r = 10$ normais que se aproxima da distribuição do erro:

\begin{equation}
  \ln\delta_t^2 \approx \xi_t \sim \soma{j = 1}{10} w_j \dnormalxt{m_j}{s_j^2}{}. \label{mvs:eq_mistura}
\end{equation}

\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}[<+->]

\item Comparação entre as distribuições de $\ln \delta_t^2$, que é o verdadeiro erro do modelo, de $\varepsilon_t$, que corresponde à aproximação ingênua, e de $\xi_t$, que é a mistura de 10 normais sugerida.
%
\begin{figure}[ht]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-comparacao-erros, eval = TRUE, echo = FALSE, opts.label = "regular_fig">>=
nm <- norMix(mu = c(1.92677, 1.34744, 0.73504, 0.02266, -0.85173, -1.97278,
                    -3.46788, -5.55246, -8.68384, -14.65000),
             sigma = sqrt(c(0.11265, 0.17788, 0.26768, 0.40611, 0.62699,
                            0.98583, 1.57469, 2.54498, 4.16591, 7.33342)),
             w = c(0.00609, 0.04775, 0.13057, 0.20674, 0.22715, 0.18842,
                   0.12047, 0.05591, 0.01575, 0.00115))

data.frame(x = c(-5, 5)) %>%
    ggplot(aes(x = x)) +
    stat_function(fun = function(x) exp((x - exp(x)) / 2) / sqrt(2 * pi),
                  aes(colour = "black")) +
    stat_function(fun = dnorm, aes(colour = "blue"),
                  args = list(sd = sqrt(4.935))) +
    stat_function(fun = dnorMix, aes(colour = "red"), linetype = "dashed",
                  args = list(obj = nm)) + labs(x = "$\\epsilon$",
                                                y = "$f(\\epsilon)$") +
    scale_colour_manual("", labels = c("$\\ln\\delta_t^2$", "$\\varepsilon_t$",
                                       "$\\xi_t$"),
                        values = c("black", "blue", "red")) +
    theme(legend.position = c(0.9, 0.75))
@
\caption{Comparação das distribuições de $\ln \delta_t^2$, $\varepsilon_t$ e $\xi_t$.}
\label{mvs:fig_comparacao_erros}
\end{minipage}
\end{figure}

\end{itemize}

\end{frame}

\subsection*{MVE2}

\begin{frame}

\begin{itemize}[<+->]

\item A equação que define a evolução da variável latente, $h_t$, em \eqref{mvs:mvs} pode ser reescrita:
%
\begin{align}
	h_t &= \mu + \phi(h_{t - 1} - \mu) + \eta_t, \notag\\
	h_t &= (1 - \phi)\mu + \phi h_{t - 1} + \eta_t. \label{mvs:eq_sistema_reescrita}
\end{align}

\item Portanto \eqref{mvs:eq_obs_reescrita_ingenua} e \eqref{mvs:eq_sistema_reescrita} definem a equação das observações e a equação do sistem, respectivamente.

\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}[<+->]

\item Assim, a equação das observações do MVE será:
%
\begin{equation}
Z_t = \ln Y_t^2 = \begin{bmatrix}1 & 0 & 1\end{bmatrix}\begin{bmatrix}h_t \\ \mu \\ \E{\ln \delta_t^2} \end{bmatrix} + \varepsilon_t =  \vetor{F}' \vetor{\theta}_t + \nu_t. \label{mvs:eq_obs_matrix}
\end{equation}

\item A equação do sistema, por sua vez, será:
  %
  \begin{equation}
	\vetor{\theta}_t = \begin{bmatrix}h_t \\ \mu \\ \E{\ln \delta_t^2} \end{bmatrix} = \begin{bmatrix}\phi & 1 - \phi & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix}h_{t - 1} \\ \mu \\ \E{\ln \delta_t^2} \end{bmatrix} + \begin{bmatrix}\eta_t \\ 0 \\ 0 \end{bmatrix} =  \vetor{G} \vetor{\theta}_{t - 1} + \vetor{\omega}_t. \label{mvs:eq_sistema_matrix}
\end{equation}

\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}[<+->]

\item O modelo de volatilidade estocástica é definido como um modelo de espaço-estado constante por:
  %
  \begin{equation}
  \begin{split}
    &\text{\textbf{Modelo de}} \\ &\text{\textbf{Espaço-Estado}}:
  \end{split}
\begin{cases}
  \ln Y_t^2 = \vetor{F}'\vetor{\theta}_t + \varepsilon_t, \\
  \vetor{\theta}_t = \vetor{G}\vetor{\theta}_{t-1} + \vetor{\omega}_t, \\
  \vetor{\theta}_0 \sim \dnormal{\vetor{m}_0}{\vetor{C}_0}{},
\end{cases} \label{mvs:mee}
\end{equation}
  
\item A quádrupla que caracteriza o modelo é:
%
  \begin{equation}
  \{\vetor{F}, \vetor{G}, V, \vetor{W}\} = \left\{\begin{bmatrix}1 \\ 0 \\ 1\end{bmatrix} , \begin{bmatrix}\phi & 1 - \phi & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{bmatrix}, \var{\varepsilon_t}, \begin{bmatrix}\sigma_\eta^2 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0\end{bmatrix}\right\}. \label{mvs:quadrupla}
\end{equation}

\end{itemize}

\end{frame}

\begin{frame}{Modelo Não Centralizado}
  
  \begin{itemize}[<+->]
    
    \item \cite{kastner} propõe uma transformação linear na variável latente, $h_t$:
      \begin{equation}
  \tilde{h}_t = \frac{h_t - \mu}{\sigma_\eta}. \label{mvs:h_tilde}
\end{equation}
      
    \item É possível, então, reescrever a equação do sistema, que passa a ser:
%
\begin{align}
  h_t &= \mu + \phi (h_{t-1} - \mu) + \eta_t, \notag \\
  h_t - \mu &= \phi (h_{t-1} - \mu) + \eta_t, \notag \\
  \pxt{\frac{h_t - \mu}{\sigma_\eta}} &= \phi \pxt{\frac{h_{t-1} - \mu}{\sigma_\eta}} + \frac{\eta_t}{\sigma_\eta}, \notag \\
  \tilde{h}_t &= \phi \tilde{h}_{t-1} + \tilde{\eta}_t. \label{mvs:eq_sistema_nc}
\end{align}

    
  \end{itemize}
  
\end{frame}

\begin{frame}
  
  \begin{itemize}[<+->]
    
    \item De modo semelhante, é possível reescrever a equação do sistema e a distribuição inicial da variável latente, o que leva ao modelo:
      %
      \begin{equation}
  \begin{split}
    &\text{\textbf{Modelo}} \\ &\text{\textbf{Não Centralizado}}:
  \end{split}
\begin{cases}
  Y_t &= e^{\frac{\mu}{2}}e^{\sigma_\eta\frac{\tilde{h}_t}{2}}\delta_t, \\
  \tilde{h}_t &= \phi \tilde{h}_{t-1} + \tilde{\eta}_t, \\
  \tilde{h}_0 &\sim \dnormalxt{0}{\frac{1}{1 - \phi^2}}{}.
\end{cases} \label{mvs:mvs_nc}
\end{equation}
    
  \end{itemize}
  
\end{frame}

\section*{Estimação}

\subsection*{Estimação1}

\begin{frame}{Proposta de Estimação dos Parâmetros do MVE}

\begin{itemize}[<+->]

\item MVE vs ARCH e GARCH.

\item \cite{bos} cita que o MVE é pouco amigável.

	\begin{itemize}
	\item ARCH e GARCH: Muitas variações do modelo, e basicamente uma maneira de se estimar os parâmetros.
	\item MVE: Basicamente uma definição do modelo, e muitas maneiras de se estimar os parâmetros.
	\end{itemize}
	
\item

\begin{table}[ht]
  \centering
  \caption{Métodos de estimação dos parâmetros do MVE e principais referências, \citep{bos}.}
  \scalebox{.6}{\begin{tabular}{lll}
    \hline
    \multicolumn{1}{c}{Método} & \multicolumn{1}{c}{Referência} & \multicolumn{1}{c}{Paradigma} \\
    \hline
    Quasi-Maximum Likelihood (QML) & \cite{harvey} & Clássico \\
    Gaussian Mixture Sampling (GMS) & \cite{kim} & Bayesiano \\
    Simulated Method of Moments (SMM) & \cite{gallant} & Clássico \\
    Importance Sampling (IS) & \cite{durbin} & Clássico \\
    Efficient Importance Sampling (EIS) & \cite{richard} & Clássico \\
    Improved Importance Sampling (IIS) & \cite{nguyen} & Clássico \\
    Single Site Sampler (SSS) & \cite{carter} & Bayesiano \\
    MultiMove Sampler (MMS) & \cite{shephard97} & Bayesiano \\
    \hline
  \end{tabular}}
  \label{mvs:tab_bos}
\end{table}

\end{itemize}

\end{frame}

\begin{frame}{Definição do Modelo Bayesiano}

\begin{itemize}[<+->]
  
  \item A estimação Bayesiana dos parâmetros do MVE consiste em determinar a distribuição \textit{a posteriori} conjunta de:
%
\begin{equation}
  \vetor{\psi} = (\mu, \phi, \sigma_\eta^2). \label{est:psi}
\end{equation}

\item Porém, é razoável supor:
  \begin{equation}
  p_{\vetor{\psi}}(\vetor{\psi}) = p_\mu(\mu)p_\phi(\phi)p_{\sigma_\eta^2}(\sigma_\eta^2). \label{est:posteriori}
\end{equation}
  
  \item Como o paradigma Bayesiano sugere, deve-se definir as distribuições \textit{a priori} dos parâmetros.
    
  \item As distribuições \textit{a priori} foram definidas conforme \cite{kastner} e \cite{kim}.

\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}[<+->]
  
  \item O nível $\mu$ da volatilidade tem seu suporte em $\mathbb{R}$, e será atribuída uma distribuição \textit{a priori}, $\pi(\mu)$, com densidade Gaussiana para esse parâmetro:
%
\begin{equation}
  \mu \sim \dnormal{a_\mu}{B_\mu}{}. \label{mvs:priori_mu}
\end{equation}


\item O parâmetro $\phi$ determina a persistência da volatilidade, e $\abs{\phi} < 1$. Seja uma nova variável $\phi_0$ cuja distribuição \textit{a priori} é $\dbeta{a_\phi}{b_\phi}{}$.

\item A persistência então será $\phi = 2\phi_0 - 1$ e sua distribuição \textit{a priori} será:
%
\begin{equation}
  \pi(\phi) = \frac{\Gamma(a_\phi + b_\phi)}{2\Gamma(a_\phi)\Gamma(b_\phi)} \pxt{\frac{1 + \phi}{2}}^{a_\phi - 1} \pxt{\frac{1 - \phi}{2}}^{b_\phi - 1}. \label{mvs:priori_phi}
\end{equation}

\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}[<+->]

\item A variância da volatilidade, $\sigma_\eta^2$, tem seus possíveis valores em $\mathbb{R}^+$. Sua distribuição \textit{a priori} será:
%
\begin{equation}
  \sigma_\eta^2 \sim \dgammaxt{\frac{1}{2}}{\frac{1}{2B_\sigma}}{}. \label{mvs:priori_sigma2}
\end{equation}

\end{itemize}

\end{frame}

\begin{frame}{Estimando $\mu$, $\phi$ e $\sigma_\eta^2$}
  
  \begin{itemize}[<+->]
    
    \item A estimação dos parâmetros será feita via MCMC.
      
    \item \cite{kastner} propõe uma estratégia chamada de \textit{Ancillarity-Sufficiency Interweaving Strategy} (ASIS).
      
    \item Algoritmo da ASIS:
\begin{enumerate}
  \item Iniciar os valores de $\mu$, $\phi$ e $\sigma_\eta^2$;
  \item Estimar a série de $h_t$ (modelo canônico);
  \item A partir dos valores de $h_t$, estimar os valores de $\mu$, $\phi$ e $\sigma_\eta^2$;
  \item Transformar a série de $h_t$ para o modelo não centralizado, $h_t^*$;
  \item A partir dos valores de $h_t^*$, estimar novamente os valores de $\mu$, $\phi$ e $\sigma_\eta^2$.
\end{enumerate}
      
  \end{itemize}
  
\end{frame}

\begin{frame}
  
  \begin{itemize}[<+->]
    
    \item Baseado em \cite{yu-meng}, \cite{kastner} diz que a variável latente, no modelo canônico, forma uma estatística suficiente para $\mu$ e $\sigma_\eta^2$. E quando $h_t$ é transformada para o modelo não centralizado, forma uma estatística ancilar.
      
    \item Alternar entre essas especificações do modelo aumenta a eficiência do amostrador via MCMC.
    
  \end{itemize}
  
\end{frame}

\begin{frame}
  
  \begin{itemize}[<+->]
    
    \item Sob o modelo canônico, os parâmetros são estimados pelo algoritmo de Metropolis-Hastings (\cite{metropolis}; \cite{hastings}).
      
    \item Geradora de candidatos:
      
      \begin{itemize}
        
        \item $\sigma_\eta^2$: Distribuição Gamma-Inversa;
        
        \item $\phi$: Distribuição Normal truncada;
          
        \item $\mu$: Distribuição Normal (amostrado indiretamente através da variável $\gamma = (1 - \phi) \mu$).
        
      \end{itemize}
      
    \item O candidato é aceito com probabilidade $\min (1, R)$.
    
  \end{itemize}
  
\end{frame}

\begin{frame}
  
  \begin{itemize}[<+->]
    
    \item A variável latente é transformada para o modelo não centralizado via:
      \begin{equation}
  \tilde{h}_t = \frac{h_t - \mu}{\sigma_\eta}. \label{est:h_tilde}
\end{equation}

      \item Os valores de $\mu$ e $\sigma_\eta^2$ são reamostrados, agora diretamente, através das novas geradoras de candidatos:
        
        \begin{itemize}
          \item $\sigma_\eta$: Distribuição Normal;
            
          \item $\mu$: Distribuição Normal.
        \end{itemize}
        
      \item O valor de $\phi$ não precisa ser reamostrado, pois esse parâmetro não é explicitamente envolvido na transformação da variável latente.
    
  \end{itemize}
  
\end{frame}

\subsection*{Estimação2}

\begin{frame}{Estimando $h_{1:N}$}
  
  \begin{itemize}[<+->]
    
    \item \cite{kastner} apresenta uma maneira de se estimar a variável latente através de uma distribuição normal $N-$variada.
      
    \item A implementação computacional desse algoritmo não é trivial.
      
    \item Proposta alternativa: estimar $h_t$ através de um algoritmo inspirado em \cite{mccormick}.
      
    \item Essa proposta é motivada no fato de ser mais simples do que a proposta original em \cite{kastner}.
    
  \end{itemize}
  
\end{frame}

\begin{frame}{O Método de \cite{mccormick}}
  
  \begin{itemize}[<+->]
    
    \item A estimação recursiva dos estados latentes, $\vetor{\theta}_t$, do modelo começa supondo que:
      \begin{equation}
  (\vetor{\theta}_{t-1} | D_{t-1}) \sim \dnormal{\hat{\vetor{\theta}}_{t-1}}{\hat{\vetor{\Sigma}}_{t-1}}{}. \label{mc:inicio}
\end{equation}

      \item A equação de predição é então descrita por:
        \begin{equation}
  (\vetor{\theta}_t | D_{t-1}) \sim \dnormal{\vetor{G}\hat{\vetor{\theta}}_{t-1}}{\vetor{R}_t}{}, \label{mc:pred}
\end{equation}
%
        em que
        %       
        \begin{equation}
  \vetor{R}_t = \frac{\vetor{G}\hat{\vetor{\Sigma}}_{t-1}\vetor{G}^T}{\lambda_t}, \label{mc:R}
\end{equation}

        \item $\lambda_t$ é um fator de desconto com valores no intervalo $(0, 1)$.
    
  \end{itemize}
  
\end{frame}

\begin{frame}
  
  \begin{itemize}[<+->]
    
    \item A distribuição \textit{a posteriori} de $(\vetor{\theta}_t | D_t)$ é obtida aproximadamente usando-se \eqref{mc:pred} e o Teorema de Bayes:
%
\begin{equation}
  p(\vetor{\theta}_t | D_t) \propto p(y_t | \vetor{\theta}_t) \dnormal{\vetor{G}\hat{\vetor{\theta}}_{t-1}}{\vetor{R}_t}{}. \label{mc:bayes}
\end{equation}

\item O lado direito de \eqref{mc:bayes} não possui forma fechada, então a estimativa de $\vetor{\theta}_t$ é dada por:
%
\begin{equation}
  \hat{\vetor{\theta}}_t = \hat{\vetor{\theta}}_{t-1} - \left[D^2 l(\hat{\vetor{\theta}}_{t-1})\right]^{-1} D l(\hat{\vetor{\theta}}_{t-1}), \label{mc:estimador}
\end{equation}
%
onde $D l(\vetor{\theta}_t)$ e $D^2 l(\vetor{\theta}_t)$ são a primeira e a segunda derivadas de $l(\vetor{\theta}_t) = \ln p(\vetor{\theta}_t | D_t)$. 
    
  \end{itemize}
  
\end{frame}

\begin{frame}
  
  \begin{itemize}[<+->]
    
    \item Para atualizar a matriz de covariância $\vetor{\Sigma}_t$, utiliza-se:
%
\begin{equation}
  \hat{\vetor{\Sigma}}_t = - \left[D^2 l(\hat{\vetor{\theta}}_{t-1})\right]^{-1}. \label{mc:matat}
\end{equation}

\item O valor de $\lambda_t$ é obtido de forma a maximizar a distribuição preditiva:
  \begin{equation}
  p(Y_t | D_{t-1}, \lambda_t) = \int p(Y_t | \vetor{\theta}_t, D_{t-1}) p(\vetor{\theta}_t | D_{t-1}, \lambda_t) d\vetor{\theta}_t. \label{mc:integralloca}
\end{equation}

  \item Os autores utilizam aproximações de Laplace \citep{tierney} para calcular o valor de \eqref{mc:integralloca}.
    
  \end{itemize}
  
\end{frame}

\begin{frame}{Adaptação do Método de \cite{mccormick} ao MVE}
  
  \begin{itemize}[<+->]
    
    \item Seja, por hipótese:
%
\begin{equation}
  (h_{t-1} | D_{t-1}) \sim \dnormal{\hat{m}_{t-1}}{\hat{C}_{t-1}}{}. \label{est:mccormick_2}
\end{equation}

\item A distribuição de predição $(h_t | D_{t-1})$ é encontrada através da equação do sistema e de \eqref{est:mccormick_2}:
  \begin{align}
  (h_t | D_{t - 1}) \sim \dnormal{\mu(1 - \phi) + \phi \hat{m}_{t-1}}{\phi^2 \hat{C}_{t-1} + \sigma_\eta^2}{}. \label{est:mccormick_3}
\end{align}
  
  \item Nesse caso:
    \begin{equation}
  R_t = \frac{\phi^2 \hat{C}_{t-1}}{\lambda_t}. \label{est:mccormick_4}
\end{equation}

    
  \end{itemize}
  
\end{frame}

\begin{frame}
  
  \begin{itemize}[<+->]
    
    \item Pelo Teorema de Bayes, a distribuição \textit{a posteriori} $(h_t | D_t)$ será:
      \begin{equation}
  p(h_t | D_t) \propto p(y_t | h_t) p(h_t | D_{t-1}). \label{est:mccormick_5}
\end{equation}

      \item O logaritmo do lado direito de \eqref{est:mccormick_5} é dado por:
        \begin{equation}
  l(h_t | \mu, \phi) \propto -\frac{h_t}{2} - \frac{y_t^2}{2e^{h_t}} - \frac{1}{2} \ln{R_t} - \frac{1}{2 R_t}\left[h_t - (\mu(1 - \phi) + \phi\hat{m}_{t-1})\right]^2. \label{est:mccormick_7}
\end{equation}

        \item Com isso, iterativamente ao longo das $N$ observações, o estimador, $\hat{h}_t$, da variável latente, $h_t$, será:
%
\begin{equation}
  \hat{h}_t = \hat{m}_t = \hat{m}_{t-1} - \frac{l'(\hat{m}_{t-1})}{l''(\hat{m}_{t-1})}.
\end{equation}

  \end{itemize}
  
\end{frame}


\begin{frame}
  
  \begin{itemize}[<+->]
    
    \item A primeira derivada é:
\begin{equation}
  l'(h_t | \mu, \phi) = -\frac{1}{2} + \frac{y_t^2}{2 e^{h_t}} - \frac{1}{R_t} \left[h_t - (\mu(1 - \phi) + \phi\hat{m}_{t-1})\right]. \label{est:d1}
\end{equation}

\item A segunda derivada, por sua vez, é:
%
\begin{equation}
  l''(h_t | \mu, \phi) = - \frac{y_t^2}{2 e^{h_t}} - \frac{1}{R_t}. \label{est:d2}
\end{equation}

\item A variância da distribuição de $h_t$ ao longo do processo é atualizada por:
%
\begin{equation}
  \hat{C}_t = -\frac{1}{l''(\hat{m}_{t-1})}. \label{est:variC}
\end{equation}
    
  \end{itemize}
  
\end{frame}

\begin{frame}
  
  \begin{itemize}[<+->]
    
    \item O valor do fator de desconto, $\lambda_t$, é calculado de modo a maximizar a distribuição preditiva $(y_t | D_{t-1})$:
%
\begin{equation}
  p(y_t | D_{t-1}, \lambda_t) = \int_{h_t} p(y_t | h_t, D_{t-1})p(h_t | D_{t-1}, \lambda_t)dh_t. \label{est:integral}
\end{equation}

\item Como a integral não possui forma fechada, o valor da distribuição preditiva é tomado por uma aproximação de Laplace.
    
  \end{itemize}
  
\end{frame}

\section*{Aplicação}

\subsection*{Aplicação1}

\begin{frame}{Aplicação da Metodologia Proposta}
  
  \begin{itemize}[<+->]
    
    \item A metodologia proposta foi aplicada num conjunto de dados simulados.
      
    \item As séries com 1.461 observações se caracterizam pelo produto cruzado dos possíveis valores predeterminados para $\mu$, $\phi$ e $\sigma_\eta^2$ a seguir:
%
\begin{itemize}
  \item $\mu = -5,4$,
  \item $\phi \in \{0,50; 0,80; 0,90; 0,99\}$,
  \item $\sigma_\eta^2 \in \{0,1^2; 0,3^2; 0,5^2\}$.
\end{itemize}
    
  \end{itemize}
  
\end{frame}

\begin{frame}
  
  \begin{figure}[ht]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-ex, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "regular_fig">>=
dados_sim <- "SELECT y AS a, h AS b
              FROM p09s05
              WHERE k = 1" %>%
             dbGetQuery(link, .) %>%
             tidyr::gather(ind, values) %>%
             mutate(t = 1:n(), t = ifelse(ind == "b", t - n() / 2, t))

dados_sim %>%
    ggplot(aes(x = t, y = values, col = ind)) +
    geom_line(size = 0.5) +
    labs(x = "$t$", y = "") +
    scale_colour_manual("", labels = c("$y_t$", "$h_t$"),
                        values = c("black", "red")) +
    theme(legend.position = c(0.9, 0.75))
@
\caption{Conjunto de dados simulados a partir do modelo canônico em \eqref{mvs:mvs}, $\mu = -5,4$, $\phi = 0,90$ e $\sigma_\eta^2 = 0,5^2$.}
\label{mvs:fig_mvs_ex}
\end{minipage}
\end{figure}

\end{frame}

\begin{frame}{Estimação dos Parâmetros}
  
  \begin{itemize}[<+->]
    
    \item O algoritimo de estimação foi implementado em C.
      
    \item Em todas as execuções foram tomadas:
      
      \begin{itemize}
        \item 15.000 iterações;
          \item \textit{burn-in} = 5.000;
            \item \textit{thin} = 10.
        \end{itemize}
      
    \item As distribuições \textit{a priori} de $\mu$, $\phi$ e $\sigma_\eta^2$ foram definadas como sugere \cite{kim} e \cite{kastner}:
%
\begin{align}
  \mu &\sim \dnormal{0}{100}{}, \notag \\
  \phi_0 &\sim \dbetaxt{20}{\frac{3}{2}}{}, \label{res:priori_mu} \\
  \sigma_\eta^2 &\sim \dgammaxt{\frac{1}{2}}{\frac{1}{2}}{}. \notag
\end{align}
    
  \end{itemize}
  
\end{frame}

\begin{frame}
  
  \begin{itemize}[<+->]
    
    \item Primeiro passo foi avaliar a performance do processo de estimação da variável latente. Para isso, os valores reais de $h_t$ foram comparados através de gráficos com os valores médios dos 1.000 valores estimados de $\hat{h}_t$:
%
\begin{equation}
  \bar{\hat{h}}_t = \frac{\soma{j = 1}{1000}\hat{h}_{t, j}}{1000}. \label{est:hmedio}
\end{equation}
    
\item Os possíveis valores do fator de desconto, $\lambda_t$, foram limitados ao intervalo (0,75; 1).

  \end{itemize}
  
\end{frame}

\begin{frame}{$\mu$, $\phi$ e $\sigma_\eta^2$ conhecidos}
  
  \begin{figure}[ht]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-res-1, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "regular_fig">>=
comp_h(1)
@
\caption{Valores médios estimados de $h_t$ do modelo \eqref{mvs:mvs} quando os parâmetros $\mu$, $\phi$ e $\sigma_\eta^2$ são conhecidos ($\mu = -5,4$, $\phi = 0,90$ e $\sigma_\eta^2 = 0,5^2$).}
\label{res:fig_1}
\end{minipage}
\end{figure}
  
\end{frame}

\begin{frame}{Apenas $\phi$ é conhecido}
\begin{figure}[ht]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-res-2, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "regular_fig">>=
comp_h(2)
@
\caption{Valores médios estimados de $h_t$ do modelo \eqref{mvs:mvs} quando apenas o parâmetro $\phi$ é conhecido ($\mu = -5,4$, $\phi = 0,90$ e $\sigma_\eta^2 = 0,5^2$).}
\label{res:fig_2}
\end{minipage}
\end{figure}
\end{frame}

\begin{frame}{$\phi$ parte do valor real}
  \begin{figure}[ht]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-res-3, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "regular_fig">>=
comp_h(3)
@
\caption{Valores médios estimados de $h_t$ do modelo \eqref{mvs:mvs} quando o valor inicial de $\phi$ é igual ao valor verdadeiro do parâmetro ($\mu = -5,4$, $\phi = 0,90$ e $\sigma_\eta^2 = 0,5^2$).}
\label{res:fig_3}
\end{minipage}
\end{figure}
\end{frame}

\begin{frame}{$\lambda_t =  0,50$}
  \begin{figure}[ht]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-res-4, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "regular_fig">>=
comp_h(4)
@
\caption{Valores médios estimados de $h_t$ do modelo \eqref{mvs:mvs} quando $\lambda_t = 0,50$ ($\mu = -5,4$, $\phi = 0,90$ e $\sigma_\eta^2 = 0,5^2$).}
\label{res:fig_4}
\end{minipage}
\end{figure}
\end{frame}

\begin{frame}
  
  \begin{itemize}[<+->]
    
    \item O algoritmo foi executado para os 12 conjuntos de dados simulados.
      
    \item O valor de $\lambda_t$ foi fixado próximo ao máximo valor possível em cada caso.
      
    \item Os valores de $\mu$ são bem estimados em todas as circunstâncias.
      
    \item Os valores estimados de $\sigma_\eta^2$ são aceitáveis quando o valor real não é muito pequeno.
      
    \item Os valores estimados de $\phi$ parecem presos num ``poço de potencial''.
    
  \end{itemize}
  
\end{frame}

\begin{frame}
  
  \begin{figure}[H]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-res-5a, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "triple_fig">>=
tmp <- "SELECT mu AS V1, phi AS V2, sigma2 AS V3
        FROM resultados
        WHERE p = 0.99 AND s = 0.5 AND k = 1" %>%
       dbGetQuery(link2, .)

ggplot(tmp, aes(x = 1:1000, y = V1)) +
    geom_line(size = 0.5) +
    labs(x = "$t$", y = "$\\mu$") +
    geom_hline(yintercept = -5.4, col = "red") -> g
plot(g)

ggplot(tmp, aes(x = V1, y = ..density..)) +
    geom_histogram(binwidth = 0.4) +
    labs(x = "$\\mu$", y = "") +
    geom_vline(xintercept = -5.4, col = "red") -> g
plot(g)
@
\caption{$\mu = -5,4$, $\phi = 0,99$ e $\sigma_\eta^2 = 0,5^2$.}
\label{res:fig_5}
\end{minipage}
\end{figure}
  
\end{frame}

\begin{frame}
  
  \begin{figure}[H]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-res-5b, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "triple_fig">>=
ggplot(tmp, aes(x = 1:1000, y = V2)) +
    geom_line(size = 0.5) +
    labs(x = "$t$", y = "$\\phi$") +
    geom_hline(yintercept = 0.99, col = "red") -> g
plot(g)

ggplot(tmp, aes(x = V2, y = ..density..)) +
    geom_histogram(binwidth = 0.001) +
    labs(x = "$\\phi$", y = "") +
    geom_vline(xintercept = 0.99, col = "red") -> g
plot(g)
@
\caption{$\mu = -5,4$, $\phi = 0,99$ e $\sigma_\eta^2 = 0,5^2$.}
\label{res:fig_5}
\end{minipage}
\end{figure}
  
\end{frame}

\begin{frame}
  
  \begin{figure}[H]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-res-5c, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "triple_fig">>=
ggplot(tmp, aes(x = 1:1000, y = V3)) +
    geom_line(size = 0.5) +
    labs(x = "$t$", y = "$\\sigma_\\eta^2$") +
    geom_hline(yintercept = 0.25, col = "red") -> g
plot(g)

ggplot(tmp, aes(x = V3, y = ..density..)) +
    geom_histogram(binwidth = 0.025) +
    labs(x = "$\\sigma_\\eta^2$", y = "") +
    geom_vline(xintercept = 0.25, col = "red") -> g
plot(g)
@
\caption{$\mu = -5,4$, $\phi = 0,99$ e $\sigma_\eta^2 = 0,5^2$.}
\label{res:fig_5}
\end{minipage}
\end{figure}
  
\end{frame}

\begin{frame}
  
  \begin{figure}[H]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-res-12a, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "triple_fig", warning = FALSE>>=
tmp <- "SELECT mu AS V1, phi AS V2, sigma2 AS V3
        FROM resultados
        WHERE p = 0.8 AND s = 0.3 AND k = 5" %>%
       dbGetQuery(link2, .)

ggplot(tmp, aes(x = 1:1000, y = V1)) +
    geom_line(size = 0.5) +
    labs(x = "$t$", y = "$\\mu$") +
    geom_hline(yintercept = -5.4, col = "red") -> g
plot(g)

ggplot(tmp, aes(x = V1, y = ..density..)) +
    geom_histogram(binwidth = 0.4) +
    labs(x = "$\\mu$", y = "") +
    geom_vline(xintercept = -5.4, col = "red") -> g
plot(g)
@
\caption{$\mu = -5,4$, $\phi = 0,80$ e $\sigma_\eta^2 = 0,3^2$.}
\label{res:fig_12}
\end{minipage}
\end{figure}
  
\end{frame}

\subsection*{Aplicação2}

\begin{frame}
  
  \begin{figure}[H]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-res-12b, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "triple_fig", warning = FALSE>>=
ggplot(tmp, aes(x = 1:1000, y = V2)) +
    geom_line(size = 0.5) +
    labs(x = "$t$", y = "$\\phi$") +
    geom_hline(yintercept = 0.80, col = "red") -> g
plot(g)

ggplot(tmp, aes(x = V2, y = ..density..)) +
    geom_histogram(binwidth = 0.001) +
    labs(x = "$\\phi$", y = "") +
    geom_vline(xintercept = 0.80, col = "red") -> g
plot(g)
@
\caption{$\mu = -5,4$, $\phi = 0,80$ e $\sigma_\eta^2 = 0,3^2$.}
\label{res:fig_12}
\end{minipage}
\end{figure}
  
\end{frame}

\begin{frame}
  
  \begin{figure}[H]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-res-12c, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "triple_fig", warning = FALSE>>=
ggplot(tmp, aes(x = 1:1000, y = V3)) +
    geom_line(size = 0.5) +
    labs(x = "$t$", y = "$\\sigma_\\eta^2$") +
    geom_hline(yintercept = 0.09, col = "red") -> g
plot(g)

ggplot(tmp, aes(x = V3, y = ..density..)) +
    geom_histogram(binwidth = 0.025) +
    labs(x = "$\\sigma_\\eta^2$", y = "") +
    geom_vline(xintercept = 0.09, col = "red") -> g
plot(g)
@
\caption{$\mu = -5,4$, $\phi = 0,80$ e $\sigma_\eta^2 = 0,3^2$.}
\label{res:fig_12}
\end{minipage}
\end{figure}
  
\end{frame}

\begin{frame}{Comparação dos Resultados com o JAGS}
  
  \begin{itemize}[<+->]
    
    \item O modelo de volatilidade estocástica foi implementado em JAGS para comparar com os resultados obtidos a partir da metodologia proposta.
      
    \item A escolha do JAGS foi motivada por dois fatores: a) desenvolvimento e manutenção do programa; b) ótima integração com o R.
      
    \item Apesar das facilidades computacionais que o JAGS proporciona, os problemas de estimação dos parâmetros persistem e são bem similares.
      
   \end{itemize}
      
\end{frame}

\begin{frame}
  
  \begin{itemize}[<+->]
    
    \item De forma semelhante, os valores de $\mu$ geraram boas estimativas em todos os casos.
      
    \item Os valores estimados de $\sigma_\eta^2$, do mesmo modo, nem sempre foram como esperado. A medida que o valor real de $\phi$ decresce, o JAGS tende a subestimar os valores da volatilidade.
      
    \item Os valores estimados de $\phi$ não ficaram ``presos'', como no caso anterior. Porém as estimativas possuem uma dispersão muito alta, e são bastante dependentes da distribuição \textit{a priori} da persistência.
    
  \end{itemize}
  
\end{frame}

\begin{frame}
  
  \begin{figure}[H]
\centering
\begin{minipage}{0.9\linewidth}
<<mvs-jags-1, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "single_fig", warning = FALSE>>=
comp_jags(0.99, 0.5, 1)
@
\caption{$\mu = -5,4$, $\phi = 0,99$ e $\sigma_\eta^2 = 0,5^2$.}
\label{res:fig_jags_1}
\end{minipage}
\end{figure}
  
\end{frame}

\begin{frame}
  
  \begin{figure}[H]
\centering
\begin{minipage}{0.9\linewidth}
<<mvs-jags-8, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "single_fig", warning = FALSE>>=
comp_jags(0.8, 0.3, 5)
@
\caption{$\mu = -5,4$, $\phi = 0,80$ e $\sigma_\eta^2 = 0,3^2$.}
\label{res:fig_jags_8}
\end{minipage}
\end{figure}
  
\end{frame}

<<base_o3, include = FALSE>>=
tmp <- readr::read_tsv("../dados/o3_c", col_names = FALSE)
@

\begin{frame}{Um Exemplo com Dados Reais}
  
  \begin{itemize}[<+->]
    
    \item \cite{achcar} propõe dois MVE para estudar a concentração de ozônio na Cidade do México.
      
    \item Os modelos propostos são multivariados (cinco regiões distintas da cidade).
      
    \item Os dados se referem a medições da média semanal do nível diário máximo de ozônio nas cinco regiões.
      
    \item O algoritmo proposto, bem como a implementação em JAGS, foram executados para uma região específica (CE).
    
  \end{itemize}
  
\end{frame}

\begin{frame}
  
\begin{figure}[H]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-o3a, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "triple_fig", warning = FALSE>>=
ggplot(tmp, aes(x = 1:1000, y = X1)) +
    geom_line(size = 0.5) +
    labs(x = "$t$", y = "$\\mu$") -> g
plot(g)

ggplot(tmp, aes(x = X1, y = ..density..)) +
    geom_histogram(binwidth = 0.4) +
    labs(x = "$\\mu$", y = "") -> g
plot(g)
@
\caption{Valores estimados do MVE aplicado aos dados reais.}
\label{res:o3}
\end{minipage}
\end{figure}
  
\end{frame}

\begin{frame}
  
\begin{figure}[H]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-o3b, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "triple_fig", warning = FALSE>>=
ggplot(tmp, aes(x = 1:1000, y = X2)) +
    geom_line(size = 0.5) +
    labs(x = "$t$", y = "$\\phi$") -> g
plot(g)

ggplot(tmp, aes(x = X2, y = ..density..)) +
    geom_histogram(binwidth = 0.001) +
    labs(x = "$\\phi$", y = "") -> g
plot(g)
@
\caption{Valores estimados do MVE aplicado aos dados reais.}
\label{res:o3}
\end{minipage}
\end{figure}
  
\end{frame}

\begin{frame}
  
\begin{figure}[H]
\centering
\begin{minipage}{0.8\linewidth}
<<mvs-o3c, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "triple_fig", warning = FALSE>>=
ggplot(tmp, aes(x = 1:1000, y = X3)) +
    geom_line(size = 0.5) +
    labs(x = "$t$", y = "$\\sigma_\\eta^2$") -> g
plot(g)

ggplot(tmp, aes(x = X3, y = ..density..)) +
    geom_histogram(binwidth = 0.025) +
    labs(x = "$\\sigma_\\eta^2$", y = "") -> g
plot(g)
@
\caption{Valores estimados do MVE aplicado aos dados reais.}
\label{res:o3}
\end{minipage}
\end{figure}
  
\end{frame}

\begin{frame}
  \begin{table}[ht]
  \centering
  \caption{Estatísticas dos valores estimados dos parâmetros do MVE aplicado aos dados reais.}
  \scalebox{1}{
  \begin{tabular}{c|rrrr}
    \hline
    Parâmetro & Média & 2,5\% & Mediana & 97,5\% \\
    \hline
<<tabela_jags, include = TRUE, echo = FALSE, results = "asis">>=
tmp %>%
    summarise(media_mu = mean(X1), inf_mu = quantile(X1, 0.025),
              mediana_mu = median(X1), sup_mu = quantile(X1, 0.975),
              media_phi = mean(X2), inf_phi = quantile(X2, 0.025),
              mediana_phi = median(X2), sup_phi = quantile(X2, 0.975),
              media_s = mean(X3), inf_s = quantile(X3, 0.025),
              mediana_s = median(X3), sup_s = quantile(X3, 0.975)) %>%
    mutate_all(funs(round(., 3))) -> tab_tmp

cat("$\\mu$")
for (i in 1:12) {
    if (i == 5) {
        cat(" \\\\\n$\\phi$ &", format(as.numeric(tab_tmp[1, i]),
                                     decimal.mark = ",", nsmall = 3))
    } else if (i == 9) {
        cat(" \\\\\n$\\sigma_\\eta^2$ &", format(as.numeric(tab_tmp[1, i]),
                                     decimal.mark = ",", nsmall = 3))
    } else {
        cat(" &", format(as.numeric(tab_tmp[1, i]),
                         decimal.mark = ",", nsmall = 3))
    }
}
cat(" \\\\")
@
    \hline
   \end{tabular}}
   \label{tab:o3}
\end{table}
  \end{frame}

<<base-jags2, include = FALSE>>=
tmp <- bind_rows(tmp, readr::read_tsv("../dados/o3_jags",
                                      col_names = FALSE)) %>%
    mutate(origem = rep(c("ASIS", "JAGS"), each = 1000))
@
  
  \begin{frame}
  
  \begin{figure}[H]
\centering
\begin{minipage}{0.9\linewidth}
<<mvs-jags-o3, include = TRUE, eval = TRUE, echo = FALSE, opts.label = "single_fig", warning = FALSE>>=
## mu
ggplot(tmp, aes(x = X1, fill = origem)) +
    geom_density(alpha = 0.4) +
    labs(x = "$\\mu$", y = "") +
    scale_fill_manual("", labels = c("ASIS", "JAGS"),
                      values = c("black", "blue")) +
    theme(legend.position = c(0.1, 0.75))

## phi
ggplot(tmp, aes(x = X2, fill = origem)) +
    geom_density(alpha = 0.4) +
    labs(x = "$\\phi$", y = "") +
    scale_fill_manual("", labels = c("ASIS", "JAGS"),
                      values = c("black", "blue")) +
    theme(legend.position = c(0.1, 0.75))

## sigma
ggplot(tmp, aes(x = X3, fill = origem)) +
    geom_density(alpha = 0.4) +
    labs(x = "$\\sigma_\\eta^2$", y = "") +
    scale_fill_manual("", labels = c("ASIS", "JAGS"),
                      values = c("black", "blue")) +
    theme(legend.position = c(0.9, 0.75))
@
\caption{Valores estimados via JAGS vs método proposto aplicados aos dados reais.}
\label{res:fig_jags_o3}
\end{minipage}
\end{figure}
  
\end{frame}

\section*{Conclusão}

\subsection*{Conclusão}

\begin{frame}
  
  \begin{itemize}[<+->]
    
    \item
    
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{Referências Bibliográficas}

\bibliographystyle{apa}
\bibliography{../mestrado}

\end{frame}

\end{document}
